{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6742c633-b46a-4a78-a870-04618c1b5658",
   "metadata": {},
   "source": [
    "## Quick reminder on Logistic Regression\n",
    "Logistic Regression is a supervised machine learning algorithm used for classification problems. Unlike linear regression which predicts continuous values it predicts the probability that an input belongs to a specific class. It is used for binary classification where the output can be one of two possible categories such as Yes/No, True/False or 0/1. It uses sigmoid function to convert inputs into a probability value between 0 and 1. In this article, we will see the basics of logistic regression and its core concepts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91be422b-1710-4576-8efb-a30521ad154b",
   "metadata": {},
   "source": [
    "In this tutorial series, we are going to cover **Logistic Regression** using **Pyspark**.\n",
    "\n",
    "**Logistic Regression** is one of the basic ways to perform classification (don’t be confused by the word “regression”). Logistic Regression is a classification method.\n",
    "\n",
    "Some examples of classification are:\n",
    "- Spam detection\n",
    "- Disease Diagnosis\n",
    "  \n",
    "\n",
    "We will be using the data for Titanic where I have columns PassengerId, Survived, Pclass, Name, Sex, Age, SibSp, Parch, Ticket, Fare, Cabin, and Embarked. **We have to predict whether the passenger will survive or not using the Logistic Regression machine learning model**. To get started, open a new notebook and follow the steps mentioned in the below code:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0227ae5-4a76-49e4-b593-d68733aa2b94",
   "metadata": {},
   "source": [
    "### 1) Import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14ce27f2-b8e5-41bc-9e1e-992768977ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "#SparkSession is now the entry point of Spark\n",
    "#SparkSession can also be construed as gateway to spark libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e44bc18-4a94-42b4-b675-45430a0e62df",
   "metadata": {},
   "source": [
    "### 2) Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5617f5ed-e497-4345-9048-047335d7be20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/10/31 15:59:37 WARN Utils: Your hostname, GERARD, resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)\n",
      "25/10/31 15:59:37 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/10/31 15:59:38 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "|PassengerId|Survived|Pclass|                Name|   Sex| Age|SibSp|Parch|          Ticket|   Fare|Cabin|Embarked|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "|          1|       0|     3|Braund, Mr. Owen ...|  male|22.0|    1|    0|       A/5 21171|   7.25| NULL|       S|\n",
      "|          2|       1|     1|Cumings, Mrs. Joh...|female|38.0|    1|    0|        PC 17599|71.2833|  C85|       C|\n",
      "|          3|       1|     3|Heikkinen, Miss. ...|female|26.0|    0|    0|STON/O2. 3101282|  7.925| NULL|       S|\n",
      "|          4|       1|     1|Futrelle, Mrs. Ja...|female|35.0|    1|    0|          113803|   53.1| C123|       S|\n",
      "|          5|       0|     3|Allen, Mr. Willia...|  male|35.0|    0|    0|          373450|   8.05| NULL|       S|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "only showing top 5 rows\n",
      "891\n"
     ]
    }
   ],
   "source": [
    "# Starting the Spark Session\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('Titanic').getOrCreate()\n",
    "\n",
    "# Reading the data\n",
    "df = spark.read.csv('titanic.csv',inferSchema=True, header=True)\n",
    "\n",
    "# Showing the data\n",
    "df.show(5)\n",
    "print(df.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c870cd4-c7e6-48c0-992c-e8eacab69ad0",
   "metadata": {},
   "source": [
    "#### Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9036f47b-1670-458d-b265-89988b3e4049",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- PassengerId: integer (nullable = true)\n",
      " |-- Survived: integer (nullable = true)\n",
      " |-- Pclass: integer (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Sex: string (nullable = true)\n",
      " |-- Age: double (nullable = true)\n",
      " |-- SibSp: integer (nullable = true)\n",
      " |-- Parch: integer (nullable = true)\n",
      " |-- Ticket: string (nullable = true)\n",
      " |-- Fare: double (nullable = true)\n",
      " |-- Cabin: string (nullable = true)\n",
      " |-- Embarked: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42b82a97-f2fa-423e-b437-986fd51072bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PassengerId',\n",
       " 'Survived',\n",
       " 'Pclass',\n",
       " 'Name',\n",
       " 'Sex',\n",
       " 'Age',\n",
       " 'SibSp',\n",
       " 'Parch',\n",
       " 'Ticket',\n",
       " 'Fare',\n",
       " 'Cabin',\n",
       " 'Embarked']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "562ccdb5-6ec8-4f57-bc36-505c244fa40d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|Survived|count|\n",
      "+--------+-----+\n",
      "|       1|  342|\n",
      "|       0|  549|\n",
      "+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('Survived').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd034174-9ceb-4f1e-a254-112beb8ec408",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+----------+\n",
      "|Survived|count|percentage|\n",
      "+--------+-----+----------+\n",
      "|       1|  342|     38.38|\n",
      "|       0|  549|     61.62|\n",
      "+--------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "df.groupBy('Survived') \\\n",
    "  .agg(F.count('*').alias('count')) \\\n",
    "  .withColumn('percentage', F.round(F.col('count') / df.count() * 100, 2)) \\\n",
    "  .show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "520b5ba6-debe-47ac-8a2d-6ae7aee4a379",
   "metadata": {},
   "source": [
    "Now it's clear that the data is realy umbalanced."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8046a586-b701-428a-af83-b43f8b229307",
   "metadata": {},
   "source": [
    "### 3) Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d486969d-28f0-46ad-90cc-e97b11c64025",
   "metadata": {},
   "source": [
    "##### Number of null values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8849a81a-26f6-4342-bbd1-1dd04ceb758d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+----+---+------------------+-----+-----+------+----+-----------------+-------------------+\n",
      "|PassengerId|Survived|Pclass|Name|Sex|               Age|SibSp|Parch|Ticket|Fare|            Cabin|           Embarked|\n",
      "+-----------+--------+------+----+---+------------------+-----+-----+------+----+-----------------+-------------------+\n",
      "|        0.0|     0.0|   0.0| 0.0|0.0|19.865319865319865|  0.0|  0.0|   0.0| 0.0|77.10437710437711|0.22446689113355783|\n",
      "+-----------+--------+------+----+---+------------------+-----+-----+------+----+-----------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Total number lines\n",
    "total_rows = df.count()\n",
    "\n",
    "# Pourcentage calculus\n",
    "pourcentage_null = df.select([\n",
    "    (F.count(F.when(F.col(c).isNull(), c)) / total_rows * 100).alias(c)\n",
    "    for c in df.columns\n",
    "])\n",
    "\n",
    "pourcentage_null.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7407f6a3-82e6-4949-aae5-0889dde41cfd",
   "metadata": {},
   "source": [
    "Removing NULL Values Columns\n",
    "\n",
    "The next step includes removing the data having null values as shown in the above picture. **We do not need the columns PassengerId, Name, Ticket, and Cabin as they are not required to train and test the model**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b751ebad-dddb-4ef1-8a77-36ee3420319b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "|PassengerId|Survived|Pclass|                Name|   Sex| Age|SibSp|Parch|          Ticket|   Fare|Cabin|Embarked|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "|          1|       0|     3|Braund, Mr. Owen ...|  male|22.0|    1|    0|       A/5 21171|   7.25| NULL|       S|\n",
      "|          2|       1|     1|Cumings, Mrs. Joh...|female|38.0|    1|    0|        PC 17599|71.2833|  C85|       C|\n",
      "|          3|       1|     3|Heikkinen, Miss. ...|female|26.0|    0|    0|STON/O2. 3101282|  7.925| NULL|       S|\n",
      "|          4|       1|     1|Futrelle, Mrs. Ja...|female|35.0|    1|    0|          113803|   53.1| C123|       S|\n",
      "|          5|       0|     3|Allen, Mr. Willia...|  male|35.0|    0|    0|          373450|   8.05| NULL|       S|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "only showing top 5 rows\n",
      "712\n"
     ]
    }
   ],
   "source": [
    "# Selecting the columns which are required \n",
    "# to train and test the model.\n",
    "\n",
    "df_clean = df.dropna(subset=['Embarked', 'Survived', 'Age', 'Fare'])\n",
    "\n",
    "# Again showing the data\n",
    "df_clean.show(5)\n",
    "print(df_clean.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f97202f-0fa9-43d8-85d2-4be56fe29207",
   "metadata": {},
   "source": [
    "#### Convert String Column to Ordinal Columns \n",
    "The next task is to convert the string columns (Sex and Embarked) to integral columns as without doing this, we cannot vectorize the data using VectorAssembler. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "178a03e4-907c-4ab3-8485-4d945a333e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the required libraries\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer, OneHotEncoder\n",
    "\n",
    "# Converting the Sex Column\n",
    "sex_indexer = StringIndexer(inputCol=\"Sex\", outputCol=\"SexIndex\", handleInvalid=\"keep\")\n",
    "sex_encoder = OneHotEncoder(inputCol=\"SexIndex\", outputCol=\"SexVec\")\n",
    "\n",
    "\n",
    "# Converting the Embarked Column\n",
    "embark_indexer = StringIndexer(inputCol=\"Embarked\", outputCol=\"EmbarkIndex\", handleInvalid=\"keep\")\n",
    "embark_encoder = OneHotEncoder(inputCol=\"EmbarkIndex\", outputCol=\"EmbarkVec\")\n",
    "\n",
    "# Vectorizing the data into a new column \"features\" \n",
    "# which will be our input/features class\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"Pclass\", \"SexVec\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"EmbarkVec\"],\n",
    "    outputCol=\"features\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203be363-8b93-4da9-9776-58c3754e51ae",
   "metadata": {},
   "source": [
    "Now we need Pipeline to stack the tasks one by one and import and call the Logistic Regression Model.\n",
    "\n",
    "**Note**: ``LogisticRegression(regParam=0.01, elasticNetParam=0.0)  # Ridge`` and ``LogisticRegression(regParam=0.01, elasticNetParam=1.0)  # Lasso``\n",
    "\n",
    "In particular, ``LogisticRegression(regParam=0.0, elasticNetParam=0.0)  # ordinary least squares``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7c20110b-2e93-49a9-9c6c-8d8ff911195d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Pipeline and Model\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# Load the logistic regression (optimized)\n",
    "log_reg = LogisticRegression(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"Survived\",\n",
    "    regParam=0.01\n",
    ")\n",
    "\n",
    "\n",
    "# Creating the pipeline\n",
    "pipeline = Pipeline(stages=[sex_indexer, embark_indexer, sex_encoder, embark_encoder, assembler, log_reg])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a5dd29-c09f-4ce8-bd89-485a1846b78f",
   "metadata": {},
   "source": [
    "After pipelining the tasks, we will split the data into training data and testing data to train and test the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "958af032-02a1-4e1e-9b3f-4c78647057d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/31 15:59:49 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+----------+--------------------+\n",
      "|            features|Survived|prediction|         probability|\n",
      "+--------------------+--------+----------+--------------------+\n",
      "|(10,[0,2,3,6,7],[...|       1|       1.0|[0.38793956061383...|\n",
      "|[3.0,1.0,0.0,2.0,...|       0|       0.0|[0.85469593420884...|\n",
      "|[2.0,0.0,1.0,14.0...|       1|       1.0|[0.11708252002681...|\n",
      "|[3.0,0.0,1.0,4.0,...|       1|       1.0|[0.26437873163312...|\n",
      "|(10,[0,2,3,6,7],[...|       0|       1.0|[0.28840096116707...|\n",
      "|(10,[0,2,3,6,7],[...|       1|       1.0|[0.39450839653089...|\n",
      "|[3.0,1.0,0.0,2.0,...|       0|       0.0|[0.92053408959047...|\n",
      "|(10,[0,2,3,6,9],[...|       1|       1.0|[0.38802353469557...|\n",
      "|[3.0,0.0,1.0,8.0,...|       0|       1.0|[0.41943696872783...|\n",
      "|[1.0,1.0,0.0,19.0...|       0|       0.0|[0.50052289937674...|\n",
      "+--------------------+--------+----------+--------------------+\n",
      "only showing top 10 rows\n"
     ]
    }
   ],
   "source": [
    "# Splitting the data into train and test\n",
    "train_data, test_data = df_clean.randomSplit([0.7, 0.3], seed=42)\n",
    "# Fitting the model on training data\n",
    "fit_model = pipeline.fit(train_data)\n",
    "\n",
    "# Storing the results on test data\n",
    "results = fit_model.transform(test_data)\n",
    "\n",
    "# Showing the results\n",
    "results.select('features', 'Survived', 'prediction', 'probability' ).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae40c10b-bb34-4509-aa50-9a55ac30db84",
   "metadata": {},
   "source": [
    "#### Model evaluation using ROC-AUC\n",
    "The results will add extra columns rawPrediction, probability, and prediction because we are transforming the results on our data. After getting the results, we will now find the AUC(Area under the ROC Curve) which will give the efficiency of the model. For this, we will use BinaryClassificationEvaluator as shown:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7b5aa4d5-61aa-4e8e-a1b0-b40d0d0b4c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the evaluator\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Calling the evaluator\n",
    "res = BinaryClassificationEvaluator(rawPredictionCol='prediction',labelCol='Survived')\n",
    "\n",
    "# Evaluating the AUC, AUCPR, F1 on results\n",
    "ROC_AUC = res.evaluate(results)\n",
    "AUCPR=BinaryClassificationEvaluator(rawPredictionCol='prediction',labelCol='Survived', metricName=\"areaUnderPR\").evaluate(results)\n",
    "f1 = MulticlassClassificationEvaluator(metricName=\"f1\", labelCol=\"Survived\").evaluate(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d812c7d2-85a6-441f-96fb-deaabcc7f5fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC  : 0.8240\n",
      "PR  AUC  : 0.7178\n",
      "F1-score : 0.8257\n"
     ]
    }
   ],
   "source": [
    "print(f\"ROC AUC  : {ROC_AUC:.4f}\")\n",
    "print(f\"PR  AUC  : {AUCPR:.4f}\")\n",
    "print(f\"F1-score : {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8fc2fb7-1316-48a5-b0d3-0b1543326bc5",
   "metadata": {},
   "source": [
    "**Note**: In general, an AUC value above 0.7 is considered good, but it's important to compare the value to the expected performance of the problem and the data to determine if it's actually good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "52af66a5-f1c5-4ef2-9937-a5bd97c077af",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aab379e-1876-49de-8276-bbdf39479781",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
